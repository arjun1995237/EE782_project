# -*- coding: utf-8 -*-
"""project_DL_2020

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sxP2NJ0DtkNnnpBRf4OWLLYbAC5j1wF
"""

import pandas as pd
import numpy as np
import os
import cv2
from  matplotlib import pyplot as plt
import matplotlib.image as mpimg
validation_split=0.2
IMG_WIDTH=256
IMG_HEIGHT=256
prob=0.3

from google.colab import drive
drive.mount('/content/drive')

def create_dataset(img_folder,noise_type="gaussian",mean=0,std=0.1,lower_thres=25):
   
    img_data_array=[]
    Y=[]
    i=0
    for file in os.listdir(img_folder):
        i+=1
        image_path= os.path.join(img_folder,  file)
        image= cv2.imread( image_path, cv2.IMREAD_GRAYSCALE)
        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)
       
        
       
        if noise_type=="gaussian":
            image=np.array(image,dtype="float32")
            noise = np.random.normal(loc=mean,scale=std,size=(IMG_HEIGHT, IMG_WIDTH));
            image=image/255
            y=(noise+image);
            
            
             
        elif noise_type=="salt_pepper":
          
            noise = np.random.randint(low=0,high=255,size=(IMG_HEIGHT, IMG_WIDTH));
            pepper=np.asarray(noise>lower_thres,dtype="uint8")
            y=image*pepper
            y=np.asarray(y,dtype="uint8")
            salt=np.asarray(noise<lower_thres,dtype="uint8")*255
            y=y+salt
            y=y/np.max(y)
            image=image/255
        

        
        img_data_array.append(image)
        Y.append(y)
        print(i)
    return img_data_array,Y
# extract the image array and class name
im_folder_chest_xray="/content/drive/MyDrive/mixed/"
im_folder_radiology="/content/drive/MyDrive/RawImage/TrainingData"
img_data,noised_data=create_dataset(im_folder_chest_xray,noise_type="salt_pepper",mean=0,std=0.4,lower_thres=190)

plt.figure(figsize=(20,20))

for i in range(5):
    image_=img_data[i*30]
    
    print(np.shape(image_))
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.array(image_),cmap="gray")

plt.figure(figsize=(20,20))
for i in range(5):
   
    noisy_=noised_data[i*30]
    
    
    print(np.shape(noisy_))
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.array(noisy_),cmap="gray",vmin=0,vmax=1)

from random import shuffle
def train_test_gen(img_data,noised_data):
  X=[]
  Y=[]
#Adding noise to image,shuffle and split it into train,test and validate
  if len(img_data)==len(noised_data):
    train_X = np.reshape(img_data, (len(img_data), IMG_HEIGHT, IMG_WIDTH,1)) 
    train_Y=np.reshape(noised_data,(len(noised_data), IMG_HEIGHT, IMG_WIDTH,1))
  else:
    print("ERROR!")

  ind_list = [i for i in range(np.shape(train_X)[0])]
  shuffle(ind_list)
  X = train_X[ind_list, :,:,:]
  Y=train_Y[ind_list, :,:,:]
#target_new = target[ind_list,]
  print(np.shape(X)[0])
  print(np.shape(Y))


  indices = np.random.permutation(np.shape(X)[0])
  print(np.floor(validation_split*len(indices)))
  valid_data_index=indices[0:int(np.floor(validation_split*len(indices)))]
  train_data_index=indices[int(np.floor(validation_split*len(indices))):]


  train_X,train_Y = X[train_data_index,:], Y[train_data_index,:]
  valid_X,valid_Y = X[valid_data_index,:], Y[valid_data_index,:]
  print(np.shape(train_X))
  print(np.shape(valid_Y))
  return train_X,train_Y,valid_X,valid_Y
train_X,train_Y,valid_X,valid_Y=train_test_gen(img_data,noised_data)

"""**Don't look at this Block**"""

def plot_noisy_and_orig(valid_Y,valid_X):
  plt.figure(figsize=(20,20))
  print("NOISY_IMAGES---Top row")
  for i in range(5):
   
    noisy_=valid_Y[i,:,:,:]
    
    
    
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.reshape(np.array(noisy_),(IMG_HEIGHT, IMG_WIDTH)),cmap="gray",vmin=0,vmax=1)

  print("NOISELESS_IMAGES---Bottom row")
  plt.figure(figsize=(20,20))
  for i in range(5):
   
    noisy_=valid_X[i,:,:,:]
    
    
    
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.reshape(np.array(noisy_),(IMG_HEIGHT, IMG_WIDTH)),cmap="gray",vmin=0,vmax=1)
plot_noisy_and_orig(valid_Y,valid_X)



from keras.initializers import orthogonal
import keras
from keras.models import Model
from keras.optimizers import Adadelta,Adam
from keras.layers import Input, Conv2D,Conv2DTranspose, MaxPool2D, UpSampling2D,BatchNormalization,LeakyReLU,Dropout,concatenate
from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint
import keras.backend as K
IMG_HEIGHT=256
IMG_WIDTH=256
def SSIM(y_true,y_pred):
  C1=(0.0001)
  C2=0.0009
  C3=0.00045
  y_pf=K.flatten(y_pred)
  y_tf=K.flatten(y_true)
  u_ytrue=K.mean(y_tf)
  u_ypred=K.mean(y_pf)
  var_ytrue=K.var(y_tf)
  var_ypred=K.var(y_pf)
  std_ytrue=K.std(y_tf)
  std_ypred=K.std(y_pf)
 
  N=K.cast((len(y_pf)-1),"float32")
  cov=K.sum((y_pf-u_ypred)*(y_tf-u_ytrue)/N)

  I=(2.0*u_ytrue*u_ypred+C1)/(K.square(u_ytrue)+K.square(u_ypred)+C1)
  C=(2.0*std_ytrue*std_ypred+C2)/(K.square(std_ytrue)+K.square(std_ypred)+C2)
  S=1.0*(cov+C3)/(std_ypred*std_ytrue+C3)

  return S*I*C

def Conv2DLayer(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):
    prefix = f'block_{block_id}_'
    x = Conv2D(filters, kernel_size=kernel, strides=strides, padding=padding,
                      kernel_initializer=kernel_init, name=prefix+'conv')(x)
    x = LeakyReLU(name=prefix+'lrelu')(x)
    x = Dropout(0.2, name=prefix+'drop')((x))
    x = BatchNormalization(name=prefix+'conv_bn')(x)
    return x

def Transpose_Conv2D(x, filters, kernel, strides, padding, block_id, kernel_init=orthogonal()):
    prefix = f'block_{block_id}_'
    x =Conv2DTranspose(filters, kernel_size=kernel, strides=strides, padding=padding,
                               kernel_initializer=kernel_init, name=prefix+'de-conv')(x)
    x = LeakyReLU(name=prefix+'lrelu')(x)
    x =Dropout(0.2, name=prefix+'drop')((x))
    x = BatchNormalization(name=prefix+'conv_bn')(x)
    return x



def AutoEncdoer(input_shape):
    inputs =Input(shape=input_shape)
    
    # 256 x 256
    conv1 = Conv2DLayer(inputs, 64, 3, strides=1, padding='same', block_id=1)
    conv2 = Conv2DLayer(conv1, 64, 3, strides=2, padding='same', block_id=2)
    
    # 128 x 128
    conv3 = Conv2DLayer(conv2, 128, 5, strides=2, padding='same', block_id=3)
    
    # 64 x 64
    conv4 = Conv2DLayer(conv3, 128, 3, strides=1, padding='same', block_id=4)
    conv5 = Conv2DLayer(conv4, 256, 5, strides=2, padding='same', block_id=5)
    
    # 32 x 32
    conv6 = Conv2DLayer(conv5, 512, 3, strides=2, padding='same', block_id=6)
    
    # 16 x 16
    deconv1 = Transpose_Conv2D(conv6, 512, 3, strides=2, padding='same', block_id=7)
    
    # 32 x 32
    skip1 = concatenate([deconv1, conv5], name='skip1')
    conv7 = Conv2DLayer(skip1, 256, 3, strides=1, padding='same', block_id=8)
    deconv2 = Transpose_Conv2D(conv7, 128, 3, strides=2, padding='same', block_id=9)
    
    # 64 x 64
    skip2 =concatenate([deconv2, conv3], name='skip2')
    conv8 = Conv2DLayer(skip2, 128, 5, strides=1, padding='same', block_id=10)
    deconv3 = Transpose_Conv2D(conv8, 64, 3, strides=2, padding='same', block_id=11)
    
    # 128 x 128
    skip3 = concatenate([deconv3, conv2], name='skip3')
    conv9 = Conv2DLayer(skip3, 64, 5, strides=1, padding='same', block_id=12)
    deconv4 = Transpose_Conv2D(conv9, 64, 3, strides=2, padding='same', block_id=13)
    
    # 256 x 256
    skip3 = concatenate([deconv4, conv1])
    conv10 = Conv2D(1, 3, strides=1, padding='same', activation='sigmoid',
                       kernel_initializer=orthogonal(), name='final_conv')(skip3)

    
    return Model(inputs=inputs, outputs=conv10)

model = AutoEncdoer((IMG_HEIGHT,IMG_WIDTH,1))
model.summary()
# model_opt = SGD(lr=0.005, decay=1-0.995, momentum=0.7, nesterov=False)

#modelchk =ModelCheckpoint(saved_weight, monitor='val_acc', verbose=1,save_best_only=True, save_weights_only=False,mode='auto',period=2)

#tensorboard = TensorBoard(log_dir=P_LOGS,histogram_freq=0,write_graph=True, write_images=True)

#csv_logger = keras.callbacks.CSVLogger(f'{P_LOGS}/keras_log.csv',append=True)
#model.fit_generator(train_noisy_batches, steps_per_epoch = train_batches.samples // batch_size,epochs=epochs,verbose=1,  validation_data=val_noisy_batches,validation_steps = train_batches.samples // batch_size,callbacks=[modelchk, tensorboard, csv_logger],use_multiprocessing=True)

model_opt = Adam(lr=0.002)

model.compile(optimizer=model_opt, loss='mse', metrics=[SSIM])

callbacks = [
    EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode="min",restore_best_weights=False),
    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),
    ModelCheckpoint('model_gauss.h5', verbose=1, save_best_only=True, save_weights_only=True,)
]


history =model.fit(train_Y,train_X,epochs=50,batch_size=32,shuffle=False,validation_data=(valid_Y, valid_X),callbacks=callbacks)



print(history.history.keys())

plt.plot(history.history['SSIM'])
plt.plot(history.history['val_SSIM'])
plt.title('model SSIM')
plt.ylabel('SSIM')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.grid()
plt.show()


# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.grid()

plt.show()

predictions = model.predict(valid_Y)

print(np.shape(predictions))

def predict_n_plot(noisy,ground):
  predictions = model.predict(noisy)
  print("prediction score is")
  print(model.evaluate(x=noisy,y=ground))
  print("NOISY_IMAGE----Top Row ")
  plt.figure(figsize=(20,20))
  for i in range(5):
   
    orig=noisy[i,:,:,:]
    orig=orig*255.0/np.max(orig)
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.reshape(np.array(orig),(256,256)),cmap="gray",vmin=0,vmax=255)
  print("Filtered image----Bottom Row ")
  plt.figure(figsize=(20,20))

  for i in range(5):
   
    filtered=predictions[i,:,:,:]
    print(np.max(filtered))
    
    ax=plt.subplot(2,5,i+1)
    plt.imshow(np.reshape(np.array(filtered),(256,256)),cmap="gray",vmin=0,vmax=1)
predict_n_plot(valid_Y,valid_X)

img_data2,noised_data2=create_dataset(im_folder_radiology,noise_type="salt_pepper",mean=0,std=0.6,lower_thres=150)
train_X2,train_Y2,valid_X2,valid_Y2=train_test_gen(img_data2,noised_data2)

predict_n_plot(valid_Y2,valid_X2)